{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd515bf3-2663-49d8-b78b-b9fcb5100dc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Script: Recreate Metastore\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Este script tem como objetivo **recriar o metastore** do Spark/Hive com base na estrutura de diretórios existente no caminho do warehouse.  \n",
    "Ele é útil em cenários onde o metastore foi perdido ou corrompido, como no caso da reinicialização do cluster, mas os dados físicos (arquivos Delta) ainda estão presentes no armazenamento DBFS.\n",
    "\n",
    "## Funcionamento\n",
    "\n",
    "O script percorre o diretório base (`/user/hive/warehouse/`), onde os databases e tabelas do Hive/Spark são armazenados fisicamente, e recria:\n",
    "\n",
    "- **Databases**, a partir de pastas com sufixo `.db`\n",
    "- **Tabelas Delta**, baseando-se nas subpastas dos databases\n",
    "\n",
    "O Spark infere o schema automaticamente para recriar as tabelas no catálogo, assumindo que os dados estão em formato **Delta Lake**.\n",
    "\n",
    "## Observações \n",
    "- O script assume que todas as tabelas são do tipo Delta Lake\n",
    "- Utiliza dbutils.fs.ls, que é específico do ambiente Databricks\n",
    "- Requer que os arquivos estejam íntegros e acessíveis no caminho do warehouse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f444af6-3dc0-4491-b548-33d861569913",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Database 'business' recriado!\n✅ Tabela 'business.d_calendario' recriada!\n✅ Tabela 'business.d_players' recriada!\n✅ Tabela 'business.d_ranking' recriada!\n✅ Tabela 'business.d_tournaments' recriada!\n✅ Tabela 'business.f_matches' recriada!\n✅ Database 'raw' recriado!\n✅ Tabela 'raw.matches' recriada!\n✅ Tabela 'raw.players' recriada!\n✅ Tabela 'raw.rankings' recriada!\n✅ Database 'staging' recriado!\n✅ Tabela 'staging.player_id_mapping' recriada!\n✅ Database 'trusted' recriado!\n❌ Falha ao recriar tabela 'trusted._delta_log': \nYou are trying to create an external table `spark_catalog`.`trusted`.`_delta_log`\nfrom `dbfs:/user/hive/warehouse/trusted.db/_delta_log` using Delta, but there is no transaction log present at\n`dbfs:/user/hive/warehouse/trusted.db/_delta_log/_delta_log`. Check the upstream job to make sure that it is writing using\nformat(\"delta\") and that the path is the root of the table.\n\nTo learn more about Delta, see dbfs:/user/hive/warehouse/trusted.db/_delta_log\n✅ Tabela 'trusted.matches' recriada!\n✅ Tabela 'trusted.players' recriada!\n✅ Tabela 'trusted.rankings' recriada!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Inicializa Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RecreateMetastore\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Caminho base onde os databases ficam salvos\n",
    "warehouse_path = \"/user/hive/warehouse/\"\n",
    "\n",
    "# Lista os databases (pastas terminando com .db)\n",
    "for item in dbutils.fs.ls(warehouse_path):\n",
    "    if item.isDir() and item.name.endswith(\".db/\"):\n",
    "        database_path = item.path\n",
    "        db_name = os.path.basename(os.path.normpath(database_path)).replace(\".db\", \"\")\n",
    "\n",
    "        # Cria o database se não existir\n",
    "        spark.sql(f\"CREATE DATABASE IF NOT EXISTS `{db_name}` LOCATION '{database_path}'\")\n",
    "        print(f\"✅ Database '{db_name}' recriado!\")\n",
    "\n",
    "        # Agora percorre as tabelas dentro do database\n",
    "        for table in dbutils.fs.ls(database_path):\n",
    "            if table.isDir():\n",
    "                table_path = table.path\n",
    "                table_name = os.path.basename(os.path.normpath(table_path))\n",
    "\n",
    "                try:\n",
    "                    # Cria a tabela Delta com base no caminho (Spark infere schema automaticamente)\n",
    "                    spark.sql(f\"\"\"\n",
    "                        CREATE TABLE IF NOT EXISTS `{db_name}`.`{table_name}`\n",
    "                        USING DELTA\n",
    "                        LOCATION '{table_path}'\n",
    "                    \"\"\")\n",
    "                    print(f\"✅ Tabela '{db_name}.{table_name}' recriada!\")\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Falha ao recriar tabela '{db_name}.{table_name}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eae3dd8-2cbc-4dee-ba18-87a4b52f6bb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- DDL da tabela: business.d_calendario --\nCREATE TABLE spark_catalog.business.d_calendario (\n  id_data INT,\n  data DATE,\n  ano INT,\n  mes INT,\n  nome_mes STRING)\nUSING delta\nLOCATION 'dbfs:/user/hive/warehouse/business.db/d_calendario'\nTBLPROPERTIES (\n  'delta.minReaderVersion' = '1',\n  'delta.minWriterVersion' = '2')\n\n\n-- DDL da tabela: business.d_players --\nCREATE TABLE spark_catalog.business.d_players (\n  player_id BIGINT,\n  player_name STRING,\n  hand STRING,\n  height DOUBLE,\n  ioc STRING,\n  date_of_birth DOUBLE)\nUSING delta\nLOCATION 'dbfs:/user/hive/warehouse/business.db/d_players'\nTBLPROPERTIES (\n  'delta.minReaderVersion' = '1',\n  'delta.minWriterVersion' = '2')\n\n\n-- DDL da tabela: business.d_ranking --\nCREATE TABLE spark_catalog.business.d_ranking (\n  player BIGINT,\n  ranking_date BIGINT,\n  rank BIGINT,\n  points DOUBLE)\nUSING delta\nLOCATION 'dbfs:/user/hive/warehouse/business.db/d_ranking'\nTBLPROPERTIES (\n  'delta.minReaderVersion' = '1',\n  'delta.minWriterVersion' = '2')\n\n\n-- DDL da tabela: business.d_tournaments --\nCREATE TABLE spark_catalog.business.d_tournaments (\n  tourney_id STRING,\n  tourney_name STRING,\n  surface STRING,\n  draw_size STRING,\n  tourney_level STRING)\nUSING delta\nLOCATION 'dbfs:/user/hive/warehouse/business.db/d_tournaments'\nTBLPROPERTIES (\n  'delta.minReaderVersion' = '1',\n  'delta.minWriterVersion' = '2')\n\n\n-- DDL da tabela: business.f_matches --\nCREATE TABLE spark_catalog.business.f_matches (\n  match_num STRING,\n  tourney_id STRING,\n  tourney_date DOUBLE,\n  winner_id DOUBLE,\n  loser_id DOUBLE,\n  best_of STRING,\n  round STRING,\n  score STRING,\n  minutes DOUBLE,\n  w_ace DOUBLE,\n  w_df DOUBLE,\n  w_svpt DOUBLE,\n  w_1stIn DOUBLE,\n  w_1stWon DOUBLE,\n  w_2ndWon DOUBLE,\n  w_SvGms DOUBLE,\n  w_bpSaved DOUBLE,\n  w_bpFaced DOUBLE,\n  l_ace DOUBLE,\n  l_df DOUBLE,\n  l_svpt DOUBLE,\n  l_1stIn DOUBLE,\n  l_1stWon DOUBLE,\n  l_2ndWon DOUBLE,\n  l_SvGms DOUBLE,\n  l_bpSaved DOUBLE,\n  l_bpFaced DOUBLE,\n  winner_rank DOUBLE,\n  winner_rank_points DOUBLE,\n  loser_rank DOUBLE,\n  loser_rank_points DOUBLE)\nUSING delta\nLOCATION 'dbfs:/user/hive/warehouse/business.db/f_matches'\nTBLPROPERTIES (\n  'delta.minReaderVersion' = '1',\n  'delta.minWriterVersion' = '2')\n\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.getActiveSession()\n",
    "\n",
    "# Listar todas as tabelas no schema 'business'\n",
    "tables = spark.catalog.listTables(\"business\")\n",
    "\n",
    "# Gerar os DDLs para verificar as tabelas do modelo final\n",
    "for table in tables:\n",
    "    table_name = table.name\n",
    "    ddl = spark.sql(f\"SHOW CREATE TABLE business.{table_name}\").collect()[0][0]\n",
    "    print(f\"-- DDL da tabela: business.{table_name} --\\n{ddl}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 968384450904391,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "recreate_metastore",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}